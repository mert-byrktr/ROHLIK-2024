{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d61ac96",
   "metadata": {
    "papermill": {
     "duration": 0.003161,
     "end_time": "2025-02-07T12:21:54.951976",
     "exception": false,
     "start_time": "2025-02-07T12:21:54.948815",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Created by <a href=\"https://github.com/yunsuxiaozi\">yunsuxiaozi</a> 2025/02/07\n",
    "\n",
    "As a beginner friendly notebook, I will use as many text descriptions as possible to provide detailed explanations of my ideas.\n",
    "\n",
    "# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">1.Import Libraries</h1></span>\n",
    "\n",
    "Due to the continuous updates of the <a href=\"https://github.com/yunsuxiaozi/Yunbase\">Yunbase</a> GitHub repository, a static version is used <a href=\"https://www.kaggle.com/code/yunsuxiaozi/yunbase\">here</a>.We are using version 16 here.\n",
    "\n",
    "This is a framework I developed for the convenience of playing competitions and avoiding writing duplicate code. You can simply understand it as automl, although its current performance is not as good as automl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d06a202",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-07T12:21:54.958234Z",
     "iopub.status.busy": "2025-02-07T12:21:54.957910Z",
     "iopub.status.idle": "2025-02-07T12:21:54.978101Z",
     "shell.execute_reply": "2025-02-07T12:21:54.977477Z"
    },
    "papermill": {
     "duration": 0.02529,
     "end_time": "2025-02-07T12:21:54.979865",
     "exception": false,
     "start_time": "2025-02-07T12:21:54.954575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_file_path = '/kaggle/input/yunbase/Yunbase/baseline.py'\n",
    "target_file_path = '/kaggle/working/baseline.py'\n",
    "with open(source_file_path, 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "with open(target_file_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5441a265",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T12:21:54.986216Z",
     "iopub.status.busy": "2025-02-07T12:21:54.985519Z",
     "iopub.status.idle": "2025-02-07T12:22:11.586149Z",
     "shell.execute_reply": "2025-02-07T12:22:11.585078Z"
    },
    "papermill": {
     "duration": 16.605799,
     "end_time": "2025-02-07T12:22:11.588172",
     "exception": false,
     "start_time": "2025-02-07T12:21:54.982373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cuml 24.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "cesium 0.12.3 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\r\n",
      "libpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\r\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "tsfresh 0.20.3 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q --requirement /kaggle/input/yunbase/Yunbase/requirements.txt  \\\n",
    "--no-index --find-links file:/kaggle/input/yunbase/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f9f4a14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T12:22:11.595248Z",
     "iopub.status.busy": "2025-02-07T12:22:11.594960Z",
     "iopub.status.idle": "2025-02-07T12:22:30.989977Z",
     "shell.execute_reply": "2025-02-07T12:22:30.989139Z"
    },
    "papermill": {
     "duration": 19.401219,
     "end_time": "2025-02-07T12:22:30.992136",
     "exception": false,
     "start_time": "2025-02-07T12:22:11.590917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from baseline import Yunbase\n",
    "import polars as pl#similar to pandas, but with better performance when dealing with large datasets.\n",
    "import pandas as pd#read csv,parquet\n",
    "import numpy as np#for scientific computation of matrices\n",
    "from datetime import datetime, timedelta\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import gc#rubbish collection\n",
    "import warnings#avoid some negligible errors\n",
    "#The filterwarnings () method is used to set warning filters, which can control the output method and level of warning information.\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random#provide some function to generate random_seed.\n",
    "#set random seed,to make sure model can be recurrented.\n",
    "def seed_everything(seed):\n",
    "    np.random.seed(seed)#numpy's random seed\n",
    "    random.seed(seed)#python built-in random seed\n",
    "seed_everything(seed=2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3cb64f",
   "metadata": {
    "papermill": {
     "duration": 0.002321,
     "end_time": "2025-02-07T12:22:30.997296",
     "exception": false,
     "start_time": "2025-02-07T12:22:30.994975",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">2.Load Data</h1></span>\n",
    "\n",
    "<a href=\"https://www.kaggle.com/code/yunsuxiaozi/rohlik-top1-solution/notebook\">Here</a> is top1 solution in last competition. The feature engineering here refers to the top 1 solution.\n",
    "\n",
    "The main task here is to read and concatenate the dataset, while also filling in some holidays that are not included in the calendar.\n",
    "\n",
    "In order to ensure consistency between CV and LB,we only choose unique_id in test_data as train_data for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecc3af7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T12:22:31.004122Z",
     "iopub.status.busy": "2025-02-07T12:22:31.003131Z",
     "iopub.status.idle": "2025-02-07T12:22:42.103835Z",
     "shell.execute_reply": "2025-02-07T12:22:42.102912Z"
    },
    "papermill": {
     "duration": 11.106217,
     "end_time": "2025-02-07T12:22:42.105963",
     "exception": false,
     "start_time": "2025-02-07T12:22:30.999746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< load dataset >\n",
      "train.shape:(4007419, 14)\n",
      "test.shape:(47021, 12)\n",
      "< only use unique_id in testset >\n",
      "< fill missing holiday >\n",
      "calendar.shape:(23016, 8)\n",
      "< merge dataset >\n",
      "weights.shape:(5390, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>date</th>\n",
       "      <th>warehouse</th>\n",
       "      <th>total_orders</th>\n",
       "      <th>sales</th>\n",
       "      <th>sell_price_main</th>\n",
       "      <th>availability</th>\n",
       "      <th>type_0_discount</th>\n",
       "      <th>type_1_discount</th>\n",
       "      <th>type_2_discount</th>\n",
       "      <th>...</th>\n",
       "      <th>winter_school_holidays</th>\n",
       "      <th>school_holidays</th>\n",
       "      <th>long_weekend</th>\n",
       "      <th>weight</th>\n",
       "      <th>product_unique_id</th>\n",
       "      <th>name</th>\n",
       "      <th>L1_category_name_en</th>\n",
       "      <th>L2_category_name_en</th>\n",
       "      <th>L3_category_name_en</th>\n",
       "      <th>L4_category_name_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4845</td>\n",
       "      <td>2024-03-10</td>\n",
       "      <td>Budapest_1</td>\n",
       "      <td>6436.0</td>\n",
       "      <td>16.34</td>\n",
       "      <td>646.26</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.925596</td>\n",
       "      <td>2375</td>\n",
       "      <td>Croissant_35</td>\n",
       "      <td>Bakery</td>\n",
       "      <td>Bakery_L2_18</td>\n",
       "      <td>Bakery_L3_83</td>\n",
       "      <td>Bakery_L4_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4845</td>\n",
       "      <td>2021-12-20</td>\n",
       "      <td>Budapest_1</td>\n",
       "      <td>6507.0</td>\n",
       "      <td>34.55</td>\n",
       "      <td>455.96</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.925596</td>\n",
       "      <td>2375</td>\n",
       "      <td>Croissant_35</td>\n",
       "      <td>Bakery</td>\n",
       "      <td>Bakery_L2_18</td>\n",
       "      <td>Bakery_L3_83</td>\n",
       "      <td>Bakery_L4_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4845</td>\n",
       "      <td>2023-04-29</td>\n",
       "      <td>Budapest_1</td>\n",
       "      <td>5463.0</td>\n",
       "      <td>34.52</td>\n",
       "      <td>646.26</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.20024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.925596</td>\n",
       "      <td>2375</td>\n",
       "      <td>Croissant_35</td>\n",
       "      <td>Bakery</td>\n",
       "      <td>Bakery_L2_18</td>\n",
       "      <td>Bakery_L3_83</td>\n",
       "      <td>Bakery_L4_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4845</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>Budapest_1</td>\n",
       "      <td>5997.0</td>\n",
       "      <td>35.92</td>\n",
       "      <td>486.41</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.925596</td>\n",
       "      <td>2375</td>\n",
       "      <td>Croissant_35</td>\n",
       "      <td>Bakery</td>\n",
       "      <td>Bakery_L2_18</td>\n",
       "      <td>Bakery_L3_83</td>\n",
       "      <td>Bakery_L4_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4845</td>\n",
       "      <td>2024-03-02</td>\n",
       "      <td>Budapest_1</td>\n",
       "      <td>6760.0</td>\n",
       "      <td>27.26</td>\n",
       "      <td>646.26</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.925596</td>\n",
       "      <td>2375</td>\n",
       "      <td>Croissant_35</td>\n",
       "      <td>Bakery</td>\n",
       "      <td>Bakery_L2_18</td>\n",
       "      <td>Bakery_L3_83</td>\n",
       "      <td>Bakery_L4_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique_id        date   warehouse  total_orders  sales  sell_price_main  \\\n",
       "0       4845  2024-03-10  Budapest_1        6436.0  16.34           646.26   \n",
       "2       4845  2021-12-20  Budapest_1        6507.0  34.55           455.96   \n",
       "3       4845  2023-04-29  Budapest_1        5463.0  34.52           646.26   \n",
       "4       4845  2022-04-01  Budapest_1        5997.0  35.92           486.41   \n",
       "5       4845  2024-03-02  Budapest_1        6760.0  27.26           646.26   \n",
       "\n",
       "   availability  type_0_discount  type_1_discount  type_2_discount  ...  \\\n",
       "0          1.00          0.00000              0.0              0.0  ...   \n",
       "2          1.00          0.00000              0.0              0.0  ...   \n",
       "3          0.96          0.20024              0.0              0.0  ...   \n",
       "4          1.00          0.00000              0.0              0.0  ...   \n",
       "5          1.00          0.00000              0.0              0.0  ...   \n",
       "\n",
       "   winter_school_holidays  school_holidays  long_weekend    weight  \\\n",
       "0                       0                0             0  1.925596   \n",
       "2                       0                0             0  1.925596   \n",
       "3                       0                0             0  1.925596   \n",
       "4                       0                0             0  1.925596   \n",
       "5                       0                0             0  1.925596   \n",
       "\n",
       "  product_unique_id          name  L1_category_name_en  L2_category_name_en  \\\n",
       "0              2375  Croissant_35               Bakery         Bakery_L2_18   \n",
       "2              2375  Croissant_35               Bakery         Bakery_L2_18   \n",
       "3              2375  Croissant_35               Bakery         Bakery_L2_18   \n",
       "4              2375  Croissant_35               Bakery         Bakery_L2_18   \n",
       "5              2375  Croissant_35               Bakery         Bakery_L2_18   \n",
       "\n",
       "   L3_category_name_en  L4_category_name_en  \n",
       "0         Bakery_L3_83          Bakery_L4_1  \n",
       "2         Bakery_L3_83          Bakery_L4_1  \n",
       "3         Bakery_L3_83          Bakery_L4_1  \n",
       "4         Bakery_L3_83          Bakery_L4_1  \n",
       "5         Bakery_L3_83          Bakery_L4_1  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"< load dataset >\")\n",
    "train=pd.read_csv(\"/kaggle/input/rohlik-sales-forecasting-challenge-v2/sales_train.csv\")\n",
    "print(f\"train.shape:{train.shape}\")\n",
    "test=pd.read_csv(\"/kaggle/input/rohlik-sales-forecasting-challenge-v2/sales_test.csv\")\n",
    "print(f\"test.shape:{test.shape}\")\n",
    "\n",
    "print(\"< only use unique_id in testset >\")\n",
    "test_id=test['unique_id'].unique()\n",
    "train=train[train['unique_id'].isin(test_id)]\n",
    "\n",
    "print(\"< fill missing holiday >\")\n",
    "calendar=pd.read_csv(\"/kaggle/input/rohlik-sales-forecasting-challenge-v2/calendar.csv\")\n",
    "czech_holiday = [ # Prague\n",
    "    (['03/31/2024', '04/09/2023', '04/17/2022', '04/04/2021', '04/12/2020'], 'Easter Day'),#loss\n",
    "    (['05/12/2024', '05/10/2020', '05/09/2021', '05/08/2022', '05/14/2023'], \"Mother Day\"), #loss\n",
    "]\n",
    "brno_holiday = [ # Brno\n",
    "    (['03/31/2024', '04/09/2023', '04/17/2022', '04/04/2021', '04/12/2020'], 'Easter Day'),#loss\n",
    "    (['05/12/2024', '05/10/2020', '05/09/2021', '05/08/2022', '05/14/2023'], \"Mother Day\"), #loss\n",
    "]\n",
    "\n",
    "budapest_holidays = []\n",
    "# Bavaria - Munich\n",
    "munich_holidays = [\n",
    "    (['03/30/2024', '04/08/2023', '04/16/2022', '04/03/2021'], 'Holy Saturday'),#loss\n",
    "    (['05/12/2024', '05/14/2023', '05/08/2022', '05/09/2021'], 'Mother Day'),#loss\n",
    "]\n",
    "\n",
    "# Hesse - Frankfurt\n",
    "frank_holidays = [\n",
    "    (['03/30/2024', '04/08/2023', '04/16/2022', '04/03/2021'], 'Holy Saturday'),#loss\n",
    "    (['05/12/2024', '05/14/2023', '05/08/2022', '05/09/2021'], 'Mother Day'),#loss\n",
    "]\n",
    "\n",
    "def fill_loss_holidays(df_fill, warehouses, holidays):\n",
    "    df = df_fill.copy()\n",
    "    for item in holidays:\n",
    "        dates, holiday_name = item\n",
    "        generated_dates = [datetime.strptime(date, '%m/%d/%Y').strftime('%Y-%m-%d') for date in dates]\n",
    "        for generated_date in generated_dates:\n",
    "            df.loc[(df['warehouse'].isin(warehouses)) & (df['date'] == generated_date), 'holiday'] = 1\n",
    "            df.loc[(df['warehouse'].isin(warehouses)) & (df['date'] == generated_date), 'holiday_name'] = holiday_name\n",
    "    #add features\n",
    "    df['long_weekend'] = ((df['shops_closed'] == 1) & (df['shops_closed'].shift(1) == 1)).astype(np.int8)\n",
    "    \n",
    "    return df\n",
    "\n",
    "calendar = fill_loss_holidays(df_fill=calendar, warehouses=['Prague_1', 'Prague_2', 'Prague_3'], holidays=czech_holiday)\n",
    "calendar = fill_loss_holidays(df_fill=calendar, warehouses=['Brno_1'], holidays=brno_holiday)\n",
    "calendar = fill_loss_holidays(df_fill=calendar, warehouses=['Munich_1'], holidays=munich_holidays)\n",
    "calendar = fill_loss_holidays(df_fill=calendar, warehouses=['Frankfurt_1'], holidays=frank_holidays)\n",
    "calendar = fill_loss_holidays(df_fill=calendar, warehouses=['Budapest_1'], holidays=budapest_holidays)\n",
    "print(f\"calendar.shape:{calendar.shape}\")\n",
    "\n",
    "print(\"< merge dataset >\")\n",
    "train=train.merge(calendar,on=['warehouse','date'],how='left')\n",
    "test=test.merge(calendar,on=['warehouse','date'],how='left')\n",
    "\n",
    "weights=pd.read_csv(\"/kaggle/input/rohlik-sales-forecasting-challenge-v2/test_weights.csv\")\n",
    "train=train.merge(weights,on='unique_id',how='left')\n",
    "print(f\"weights.shape:{weights.shape}\")\n",
    "\n",
    "inventory=pd.read_csv(\"/kaggle/input/rohlik-sales-forecasting-challenge-v2/inventory.csv\")\n",
    "train=train.merge(inventory,on=['warehouse','unique_id'],how='left')\n",
    "test=test.merge(inventory,on=['warehouse','unique_id'],how='left')\n",
    "\n",
    "train=train[train['date']>='2021-06-01']\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c284f26c",
   "metadata": {
    "papermill": {
     "duration": 0.003324,
     "end_time": "2025-02-07T12:22:42.113883",
     "exception": false,
     "start_time": "2025-02-07T12:22:42.110559",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">3.Feature Engineer</h1></span>\n",
    "\n",
    "Here,weekend is considered a holiday.I have conducted a more detailed classification of food and constructed shift and diff features.\n",
    "\n",
    "We have done feature engineering on 'date' here, and also used the top 1 solution of Playground.<a href=\"https://www.kaggle.com/code/ivyzang/1st-place-solution-less-is-more/notebook\">less is more</a>\n",
    "\n",
    "\n",
    "The feature engineering here is quite messy. If you are interested, you can take a closer look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ef3533b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T12:22:42.121230Z",
     "iopub.status.busy": "2025-02-07T12:22:42.120902Z",
     "iopub.status.idle": "2025-02-07T12:22:42.471510Z",
     "shell.execute_reply": "2025-02-07T12:22:42.470737Z"
    },
    "papermill": {
     "duration": 0.356425,
     "end_time": "2025-02-07T12:22:42.473307",
     "exception": false,
     "start_time": "2025-02-07T12:22:42.116882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< get weekend date >\n"
     ]
    }
   ],
   "source": [
    "print(\"< get weekend date >\")\n",
    "start_date_str = train['date'].min()\n",
    "end_date_str = train['date'].max()\n",
    "start_date = datetime.strptime(start_date_str, '%Y-%m-%d')\n",
    "end_date = datetime.strptime(end_date_str, '%Y-%m-%d')\n",
    "current_date = start_date\n",
    "weekends = []\n",
    "while current_date <= end_date:\n",
    "    if current_date.weekday() == 5 or current_date.weekday() == 6:\n",
    "        weekends.append(current_date.strftime('%Y-%m-%d'))\n",
    "    current_date += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9918b9",
   "metadata": {
    "papermill": {
     "duration": 0.003019,
     "end_time": "2025-02-07T12:22:42.479769",
     "exception": false,
     "start_time": "2025-02-07T12:22:42.476750",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In the autoregressive features, 14, 20, 28, and 35 are features that have a correlation with sales greater than 0.93 within one year, while 356,364,370 are features that have a correlation greater than 0.92 after one year, and cannot even reach 0.90 after two years, so they are not considered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d6bc54c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T12:22:42.487349Z",
     "iopub.status.busy": "2025-02-07T12:22:42.487053Z",
     "iopub.status.idle": "2025-02-07T12:25:40.632465Z",
     "shell.execute_reply": "2025-02-07T12:25:40.631637Z"
    },
    "papermill": {
     "duration": 178.155801,
     "end_time": "2025-02-07T12:25:40.638636",
     "exception": false,
     "start_time": "2025-02-07T12:22:42.482835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< autoregression feature >\n",
      "< date feature >\n",
      "< data clean >\n",
      "< store2country feature >\n",
      "< add holiday >\n",
      "< add weekend feature >\n",
      "< time diff and shift feature >\n",
      "train.shape:(2938869, 149),test.shape:(47021, 147)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>date</th>\n",
       "      <th>warehouse</th>\n",
       "      <th>total_orders</th>\n",
       "      <th>sales</th>\n",
       "      <th>sell_price_main</th>\n",
       "      <th>type_0_discount</th>\n",
       "      <th>type_1_discount</th>\n",
       "      <th>type_2_discount</th>\n",
       "      <th>type_3_discount</th>\n",
       "      <th>...</th>\n",
       "      <th>median_total_type_discount_each_name_WU_per_day</th>\n",
       "      <th>median_total_type_discount_each_name0_WU_per_day</th>\n",
       "      <th>median_total_type_discount_each_L1_WU_per_day</th>\n",
       "      <th>median_total_type_discount_each_name0_W_per_day</th>\n",
       "      <th>median_total_type_discount_each_name0_per_day</th>\n",
       "      <th>median_total_type_discount_each_name_WU_per_day_diff1</th>\n",
       "      <th>median_total_type_discount_each_name0_WU_per_day_diff1</th>\n",
       "      <th>median_total_type_discount_each_L1_WU_per_day_diff1</th>\n",
       "      <th>median_total_type_discount_each_name0_W_per_day_diff1</th>\n",
       "      <th>median_total_type_discount_each_name0_per_day_diff1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4845</td>\n",
       "      <td>2024-03-10</td>\n",
       "      <td>Budapest_1</td>\n",
       "      <td>6436.0</td>\n",
       "      <td>123.624954</td>\n",
       "      <td>646.26</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.15312</td>\n",
       "      <td>0.15312</td>\n",
       "      <td>0.15312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4845</td>\n",
       "      <td>2021-12-20</td>\n",
       "      <td>Budapest_1</td>\n",
       "      <td>6507.0</td>\n",
       "      <td>244.173815</td>\n",
       "      <td>455.96</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.15025</td>\n",
       "      <td>0.15025</td>\n",
       "      <td>0.15025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4845</td>\n",
       "      <td>2023-04-29</td>\n",
       "      <td>Budapest_1</td>\n",
       "      <td>5463.0</td>\n",
       "      <td>255.398671</td>\n",
       "      <td>646.26</td>\n",
       "      <td>0.20024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.35336</td>\n",
       "      <td>0.35336</td>\n",
       "      <td>0.35336</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4845</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>Budapest_1</td>\n",
       "      <td>5997.0</td>\n",
       "      <td>220.202396</td>\n",
       "      <td>486.41</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.15649</td>\n",
       "      <td>0.15649</td>\n",
       "      <td>0.15649</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4845</td>\n",
       "      <td>2024-03-02</td>\n",
       "      <td>Budapest_1</td>\n",
       "      <td>6760.0</td>\n",
       "      <td>201.685045</td>\n",
       "      <td>646.26</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.15312</td>\n",
       "      <td>0.15312</td>\n",
       "      <td>0.15312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 149 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique_id        date   warehouse  total_orders       sales  \\\n",
       "0       4845  2024-03-10  Budapest_1        6436.0  123.624954   \n",
       "1       4845  2021-12-20  Budapest_1        6507.0  244.173815   \n",
       "2       4845  2023-04-29  Budapest_1        5463.0  255.398671   \n",
       "3       4845  2022-04-01  Budapest_1        5997.0  220.202396   \n",
       "4       4845  2024-03-02  Budapest_1        6760.0  201.685045   \n",
       "\n",
       "   sell_price_main  type_0_discount  type_1_discount  type_2_discount  \\\n",
       "0           646.26          0.00000              0.0              0.0   \n",
       "1           455.96          0.00000              0.0              0.0   \n",
       "2           646.26          0.20024              0.0              0.0   \n",
       "3           486.41          0.00000              0.0              0.0   \n",
       "4           646.26          0.00000              0.0              0.0   \n",
       "\n",
       "   type_3_discount  ...  median_total_type_discount_each_name_WU_per_day  \\\n",
       "0              0.0  ...                                          0.15312   \n",
       "1              0.0  ...                                          0.15025   \n",
       "2              0.0  ...                                          0.35336   \n",
       "3              0.0  ...                                          0.15649   \n",
       "4              0.0  ...                                          0.15312   \n",
       "\n",
       "   median_total_type_discount_each_name0_WU_per_day  \\\n",
       "0                                           0.15312   \n",
       "1                                           0.15025   \n",
       "2                                           0.35336   \n",
       "3                                           0.15649   \n",
       "4                                           0.15312   \n",
       "\n",
       "   median_total_type_discount_each_L1_WU_per_day  \\\n",
       "0                                        0.15312   \n",
       "1                                        0.15025   \n",
       "2                                        0.35336   \n",
       "3                                        0.15649   \n",
       "4                                        0.15312   \n",
       "\n",
       "  median_total_type_discount_each_name0_W_per_day  \\\n",
       "0                                             0.0   \n",
       "1                                             0.0   \n",
       "2                                             0.0   \n",
       "3                                             0.0   \n",
       "4                                             0.0   \n",
       "\n",
       "   median_total_type_discount_each_name0_per_day  \\\n",
       "0                                            0.0   \n",
       "1                                            0.0   \n",
       "2                                            0.0   \n",
       "3                                            0.0   \n",
       "4                                            0.0   \n",
       "\n",
       "   median_total_type_discount_each_name_WU_per_day_diff1  \\\n",
       "0                                                0.0       \n",
       "1                                                0.0       \n",
       "2                                                0.0       \n",
       "3                                                0.0       \n",
       "4                                                0.0       \n",
       "\n",
       "   median_total_type_discount_each_name0_WU_per_day_diff1  \\\n",
       "0                                                0.0        \n",
       "1                                                0.0        \n",
       "2                                                0.0        \n",
       "3                                                0.0        \n",
       "4                                                0.0        \n",
       "\n",
       "   median_total_type_discount_each_L1_WU_per_day_diff1  \\\n",
       "0                                                0.0     \n",
       "1                                                0.0     \n",
       "2                                                0.0     \n",
       "3                                                0.0     \n",
       "4                                                0.0     \n",
       "\n",
       "   median_total_type_discount_each_name0_W_per_day_diff1  \\\n",
       "0                                                0.0       \n",
       "1                                                0.0       \n",
       "2                                                0.0       \n",
       "3                                                0.0       \n",
       "4                                                0.0       \n",
       "\n",
       "   median_total_type_discount_each_name0_per_day_diff1  \n",
       "0                                                0.0    \n",
       "1                                                0.0    \n",
       "2                                                0.0    \n",
       "3                                                0.0    \n",
       "4                                                0.0    \n",
       "\n",
       "[5 rows x 149 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def FE(df):\n",
    "    df['index']=np.arange(len(df))\n",
    "    df=df.sort_values(['date']).reset_index(drop=True)\n",
    "\n",
    "    print(\"< autoregression feature >\")\n",
    "    for gap in [14,20,28,35,356,364,370]:\n",
    "        df[f'sales_shift{gap}']=df.groupby(['warehouse','name'])['sales'].shift(gap)\n",
    "    \n",
    "    print(\"< date feature >\")\n",
    " \n",
    "    df['date_copy']=df['date']\n",
    "    df['date_copy']=pd.to_datetime(df['date_copy'])\n",
    "    \n",
    "    df['dayofyear']=df['date_copy'].dt.dayofyear\n",
    "    df['sin_dayofyear']=np.sin(2*np.pi*df['dayofyear']/365)\n",
    "    df['cos_dayofyear']=np.cos(2*np.pi*df['dayofyear']/365)\n",
    "    \n",
    "    df['dayofweek']=df['date_copy'].dt.dayofweek\n",
    "    df['weekday'] = df['date_copy'].dt.weekday\n",
    "    df['weekend']=(df['dayofweek']>4).astype(np.int8)\n",
    "    df['sin_dayofweek']=np.sin(2*np.pi*df['dayofweek']/7)\n",
    "    df['cos_dayofweek']=np.cos(2*np.pi*df['dayofweek']/7)\n",
    "\n",
    "    dayofweek2mean={0: 0.1414975636804815,1: 0.13738876781429193,\n",
    "    2: 0.14013498532762625,3: 0.15052082748144407,4: 0.16312265716870394,\n",
    "    5: 0.13516123608364708,6: 0.13217396244380525}\n",
    "    df['dayofweek']=df['dayofweek'].apply(lambda x:dayofweek2mean[x])\n",
    "\n",
    "    df['weekofyear'] = df['date_copy'].dt.isocalendar().week\n",
    "    df['sin_weekofyear']=np.sin(2*np.pi*df['weekofyear']/52)\n",
    "    df['cos_weekofyear']=np.cos(2*np.pi*df['weekofyear']/52)\n",
    "\n",
    "    df['year']=df['date_copy'].dt.year\n",
    "    df['quarter']=df['date_copy'].dt.quarter\n",
    "    df['sin_quarter']=np.sin(2*np.pi*df['quarter']/4)\n",
    "    df['cos_quarter']=np.cos(2*np.pi*df['quarter']/4)\n",
    "    \n",
    "    df['month']=df['date_copy'].dt.month\n",
    "    df['is_month_start'] = df['date_copy'].dt.is_month_start\n",
    "    df['is_month_end'] = df['date_copy'].dt.is_month_end\n",
    "    df['sin_month']=np.sin(2*np.pi*df['month']/12)\n",
    "    df['cos_month']=np.cos(2*np.pi*df['month']/12)\n",
    "    \n",
    "    df['day']=df['date_copy'].dt.day\n",
    "    df['dayofmonth']=df['day']//10\n",
    "    df['sin_day']=np.sin(2*np.pi*df['day']/30)\n",
    "    df['cos_day']=np.cos(2*np.pi*df['day']/30)\n",
    "\n",
    "    print(\"< data clean >\")\n",
    "    #name:'Pastry_196'\n",
    "    df['name_0']=df['name'].apply(lambda x:x.split(\"_\")[0])\n",
    "    df['name_1']=df['name'].apply(lambda x:x.split(\"_\")[1])\n",
    "    df.drop(['name'],axis=1,inplace=True)\n",
    "    for i in range(2,5):\n",
    "        df[f'L{i}_category_name_en']=df[f'L{i}_category_name_en'].apply(lambda x:x.split('_')[2])\n",
    "\n",
    "    print(\"< store2country feature >\")\n",
    "    store2country = {\n",
    "        'Budapest_1': 'Hungary',\n",
    "        'Prague_2': 'Czechia',\n",
    "        'Brno_1': 'Czechia',\n",
    "        'Prague_1': 'Czechia',\n",
    "        'Prague_3': 'Czechia',\n",
    "        'Munich_1': 'Germany',\n",
    "        'Frankfurt_1': 'Germany'\n",
    "    }\n",
    "    df['country']=df['warehouse'].apply(lambda x:store2country[x])\n",
    "\n",
    "    print(\"< add holiday >\")\n",
    "    #rohlik top1 solution:https://www.kaggle.com/code/yunsuxiaozi/rohlik-top1-solution/notebook\n",
    "    rename_dict = {\n",
    "        \"Memorial Day for the Victims of the Holocaust\": \"Victims of the Holocaust\",\n",
    "        \"Memorial Day for the Victims of the Communist Dictatorships\": \"Victims of the Communist\",\n",
    "        \"Den vzniku samostatneho ceskoslovenskeho statu\": \"Den vzniku\"\n",
    "    }\n",
    "    df['holiday_name'] = df['holiday_name'].replace(rename_dict)\n",
    "    df.loc[(df['holiday']==1)&(df['holiday_name'].isna()),'holiday_name']='Easter Monday'\n",
    "    datesx = ['03/31/2024', '04/09/2023', '04/17/2022', '04/04/2021', '04/12/2020']\n",
    "    holidaysx = [datetime.strptime(date, '%m/%d/%Y') - timedelta(days=1) for date in datesx]\n",
    "    warehouses = ['Prague_1', 'Prague_2', 'Prague_3']\n",
    "    df.loc[(df['date'].isin(holidaysx)) & (df['warehouse'].isin(warehouses)), 'holiday'] = 1\n",
    "\n",
    "    print(\"< add weekend feature >\")\n",
    "    df.loc[(df['holiday_name'].isna())&(df['date'].isin(weekends)),'holiday_name']='weekend'\n",
    "    #simple weekend\n",
    "    df['is_holiday']=(df['holiday_name']==df['holiday_name']).astype(np.int8)\n",
    "    #holiday but not weekend\n",
    "    df.loc[(df['is_holiday']==1)&(df['holiday_name']!='weekend'),'is_holiday']=2\n",
    "    \n",
    "    df['total_type_discount']=0\n",
    "    for i in range(7):\n",
    "        df['total_type_discount']+=df[f'type_{i}_discount']\n",
    "\n",
    "    print(\"< time diff and shift feature >\")\n",
    "    \n",
    "    for gap in [1,2]:\n",
    "        for col in ['is_holiday','weekend']:\n",
    "            df[col+f\"_shift{gap}\"]=df.groupby(['warehouse','unique_id','product_unique_id'])[col].shift(gap)\n",
    "\n",
    "    for col in ['total_orders','sell_price_main','total_type_discount']:#'total_orders*sell_price_main'\n",
    "        for agg in ['std','skew','max','median']:\n",
    "            df[f'{agg}_{col}_each_name_WU_per_day']=df.groupby(['date','warehouse','unique_id','name_0','name_1'])[col].transform(agg)\n",
    "            df[f'{agg}_{col}_each_name0_WU_per_day']=df.groupby(['date','warehouse','unique_id','name_0'])[col].transform(agg)\n",
    "            df[f'{agg}_{col}_each_L1_WU_per_day']=df.groupby(['date','warehouse','unique_id','L1_category_name_en'])[col].transform(agg)\n",
    "            df[f'{agg}_{col}_each_name0_W_per_day']=df.groupby(['date','warehouse','name_0'])[col].transform(agg)\n",
    "            df[f'{agg}_{col}_each_name0_per_day']=df.groupby(['date','name_0'])[col].transform(agg)\n",
    "            \n",
    "            for gap in [1]:\n",
    "                df[f'{agg}_{col}_each_name_WU_per_day_diff{gap}']=df.groupby(['warehouse','unique_id','name_0','name_1'])[f'{agg}_{col}_each_name_WU_per_day'].diff(gap)\n",
    "                df[f'{agg}_{col}_each_name0_WU_per_day_diff{gap}']=df.groupby(['warehouse','unique_id','name_0','name_1'])[f'{agg}_{col}_each_name0_WU_per_day'].diff(gap)\n",
    "                df[f'{agg}_{col}_each_L1_WU_per_day_diff{gap}']=df.groupby(['warehouse','unique_id','name_0','name_1'])[f'{agg}_{col}_each_L1_WU_per_day'].diff(gap)\n",
    "                df[f'{agg}_{col}_each_name0_W_per_day_diff{gap}']=df.groupby(['warehouse','unique_id','name_0','name_1'])[f'{agg}_{col}_each_name0_W_per_day'].diff(gap)\n",
    "                df[f'{agg}_{col}_each_name0_per_day_diff{gap}']=df.groupby(['warehouse','unique_id','name_0','name_1'])[f'{agg}_{col}_each_name0_per_day'].diff(gap)\n",
    "                \n",
    "    df=df.sort_values(['index']).reset_index(drop=True)\n",
    "    \n",
    "    df.drop(['index','date_copy'],axis=1,inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "total=pd.concat((train,test))\n",
    "total=FE(total)\n",
    "#baside target,columns in train.columns but not in test.columns\n",
    "drop_cols=['availability']\n",
    "total.drop([col for col in total.columns if total[col].isna().mean()>0.99]+drop_cols,axis=1,inplace=True)\n",
    "train=total[:len(train)]\n",
    "test=total[len(train):].drop(['sales','weight'],axis=1)\n",
    "del total\n",
    "gc.collect()\n",
    "train['sales']=train['sales']/train['dayofweek']\n",
    "\n",
    "print(f\"train.shape:{train.shape},test.shape:{test.shape}\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa89e51",
   "metadata": {
    "papermill": {
     "duration": 0.003515,
     "end_time": "2025-02-07T12:25:40.645999",
     "exception": false,
     "start_time": "2025-02-07T12:25:40.642484",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">4.Fit and Predict</h1></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c15e9a63",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-02-07T12:25:40.654970Z",
     "iopub.status.busy": "2025-02-07T12:25:40.654252Z",
     "iopub.status.idle": "2025-02-07T12:53:49.141389Z",
     "shell.execute_reply": "2025-02-07T12:53:49.140476Z"
    },
    "papermill": {
     "duration": 1688.499469,
     "end_time": "2025-02-07T12:53:49.149065",
     "exception": false,
     "start_time": "2025-02-07T12:25:40.649596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently supported metrics:['custom_metric', 'mae', 'rmse', 'mse', 'medae', 'rmsle', 'msle', 'mape', 'r2', 'smape', 'auc', 'pr_auc', 'logloss', 'f1_score', 'mcc', 'accuracy', 'multi_logloss']\n",
      "Currently supported models:['lgb', 'cat', 'xgb', 'ridge', 'Lasso', 'LinearRegression', 'LogisticRegression', 'tabnet', 'Word2Vec', 'tfidfvec', 'countvec']\n",
      "Currently supported kfolds:['KFold', 'GroupKFold', 'StratifiedKFold', 'StratifiedGroupKFold', 'purged_CV']\n",
      "Currently supported objectives:['binary', 'multi_class', 'regression']\n",
      "< preprocess date_col >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< one hot encoder >\n",
      "\u001b[31m-> for column unique_id labelencoder feature\u001b[0m\n",
      "\u001b[31m-> for column L2_category_name_en labelencoder feature\u001b[0m\n",
      "\u001b[31m-> for column L3_category_name_en labelencoder feature\u001b[0m\n",
      "\u001b[31m-> for column L4_category_name_en labelencoder feature\u001b[0m\n",
      "\u001b[31m-> for column name_0 labelencoder feature\u001b[0m\n",
      "\u001b[31m-> for column name_1 labelencoder feature\u001b[0m\n",
      "\u001b[31m-> for column product_unique_id labelencoder feature\u001b[0m\n",
      "< drop high correlation feature >\n",
      "drop_cols=['max_total_orders_each_name_WU_per_day', 'max_total_orders_each_name0_WU_per_day', 'max_total_orders_each_L1_WU_per_day', 'max_total_orders_each_name0_W_per_day', 'median_total_orders_each_name_WU_per_day', 'median_total_orders_each_name0_WU_per_day', 'median_total_orders_each_L1_WU_per_day', 'median_total_orders_each_name0_W_per_day', 'max_sell_price_main_each_name_WU_per_day', 'max_sell_price_main_each_name0_WU_per_day', 'max_sell_price_main_each_L1_WU_per_day', 'median_sell_price_main_each_name_WU_per_day', 'median_sell_price_main_each_name0_WU_per_day', 'median_sell_price_main_each_L1_WU_per_day', 'month', 'sin_weekofyear', 'cos_weekofyear', 'max_total_type_discount_each_name_WU_per_day', 'max_total_type_discount_each_name0_WU_per_day', 'max_total_type_discount_each_L1_WU_per_day', 'median_total_type_discount_each_name_WU_per_day', 'median_total_type_discount_each_name0_WU_per_day', 'median_total_type_discount_each_L1_WU_per_day', 'max_total_orders_each_name0_WU_per_day_diff1', 'max_total_orders_each_L1_WU_per_day_diff1', 'median_total_orders_each_name_WU_per_day_diff1', 'median_total_orders_each_name0_WU_per_day_diff1', 'median_total_orders_each_L1_WU_per_day_diff1', 'max_sell_price_main_each_name0_WU_per_day_diff1', 'max_sell_price_main_each_L1_WU_per_day_diff1', 'median_sell_price_main_each_name_WU_per_day_diff1', 'median_sell_price_main_each_name0_WU_per_day_diff1', 'median_sell_price_main_each_L1_WU_per_day_diff1', 'max_total_type_discount_each_name0_WU_per_day_diff1', 'max_total_type_discount_each_L1_WU_per_day_diff1', 'median_total_type_discount_each_name_WU_per_day_diff1', 'median_total_type_discount_each_name0_WU_per_day_diff1', 'median_total_type_discount_each_L1_WU_per_day_diff1', 'country_Hungary', 'country_valuecounts']\n",
      "< cross feature >\n",
      "< drop useless cols >\n",
      "nan_cols:[]\n",
      "unique_cols:['type_1_discount', 'type_2_discount', 'type_3_discount', 'type_5_discount', 'school_holidays']\n",
      "drop_cols:[]\n",
      "high_corr_cols:['max_total_orders_each_name_WU_per_day', 'max_total_orders_each_name0_WU_per_day', 'max_total_orders_each_L1_WU_per_day', 'max_total_orders_each_name0_W_per_day', 'median_total_orders_each_name_WU_per_day', 'median_total_orders_each_name0_WU_per_day', 'median_total_orders_each_L1_WU_per_day', 'median_total_orders_each_name0_W_per_day', 'max_sell_price_main_each_name_WU_per_day', 'max_sell_price_main_each_name0_WU_per_day', 'max_sell_price_main_each_L1_WU_per_day', 'median_sell_price_main_each_name_WU_per_day', 'median_sell_price_main_each_name0_WU_per_day', 'median_sell_price_main_each_L1_WU_per_day', 'month', 'sin_weekofyear', 'cos_weekofyear', 'max_total_type_discount_each_name_WU_per_day', 'max_total_type_discount_each_name0_WU_per_day', 'max_total_type_discount_each_L1_WU_per_day', 'median_total_type_discount_each_name_WU_per_day', 'median_total_type_discount_each_name0_WU_per_day', 'median_total_type_discount_each_L1_WU_per_day', 'max_total_orders_each_name0_WU_per_day_diff1', 'max_total_orders_each_L1_WU_per_day_diff1', 'median_total_orders_each_name_WU_per_day_diff1', 'median_total_orders_each_name0_WU_per_day_diff1', 'median_total_orders_each_L1_WU_per_day_diff1', 'max_sell_price_main_each_name0_WU_per_day_diff1', 'max_sell_price_main_each_L1_WU_per_day_diff1', 'median_sell_price_main_each_name_WU_per_day_diff1', 'median_sell_price_main_each_name0_WU_per_day_diff1', 'median_sell_price_main_each_L1_WU_per_day_diff1', 'max_total_type_discount_each_name0_WU_per_day_diff1', 'max_total_type_discount_each_L1_WU_per_day_diff1', 'median_total_type_discount_each_name_WU_per_day_diff1', 'median_total_type_discount_each_name0_WU_per_day_diff1', 'median_total_type_discount_each_L1_WU_per_day_diff1', 'country_Hungary', 'country_valuecounts']\n",
      "Memory usage of dataframe is 2413.38 MB\n",
      "Memory usage after optimization is: 1283.89 MB\n",
      "Decreased by 46.8%\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< one hot encoder >\n",
      "\u001b[31m-> for column unique_id labelencoder feature\u001b[0m\n",
      "\u001b[31m-> for column L2_category_name_en labelencoder feature\u001b[0m\n",
      "\u001b[31m-> for column L3_category_name_en labelencoder feature\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m-> for column L4_category_name_en labelencoder feature\u001b[0m\n",
      "\u001b[31m-> for column name_0 labelencoder feature\u001b[0m\n",
      "\u001b[31m-> for column name_1 labelencoder feature\u001b[0m\n",
      "\u001b[31m-> for column product_unique_id labelencoder feature\u001b[0m\n",
      "< cross feature >\n",
      "< drop useless cols >\n",
      "nan_cols:[]\n",
      "unique_cols:['type_1_discount', 'type_2_discount', 'type_3_discount', 'type_5_discount', 'school_holidays']\n",
      "drop_cols:[]\n",
      "high_corr_cols:['max_total_orders_each_name_WU_per_day', 'max_total_orders_each_name0_WU_per_day', 'max_total_orders_each_L1_WU_per_day', 'max_total_orders_each_name0_W_per_day', 'median_total_orders_each_name_WU_per_day', 'median_total_orders_each_name0_WU_per_day', 'median_total_orders_each_L1_WU_per_day', 'median_total_orders_each_name0_W_per_day', 'max_sell_price_main_each_name_WU_per_day', 'max_sell_price_main_each_name0_WU_per_day', 'max_sell_price_main_each_L1_WU_per_day', 'median_sell_price_main_each_name_WU_per_day', 'median_sell_price_main_each_name0_WU_per_day', 'median_sell_price_main_each_L1_WU_per_day', 'month', 'sin_weekofyear', 'cos_weekofyear', 'max_total_type_discount_each_name_WU_per_day', 'max_total_type_discount_each_name0_WU_per_day', 'max_total_type_discount_each_L1_WU_per_day', 'median_total_type_discount_each_name_WU_per_day', 'median_total_type_discount_each_name0_WU_per_day', 'median_total_type_discount_each_L1_WU_per_day', 'max_total_orders_each_name0_WU_per_day_diff1', 'max_total_orders_each_L1_WU_per_day_diff1', 'median_total_orders_each_name_WU_per_day_diff1', 'median_total_orders_each_name0_WU_per_day_diff1', 'median_total_orders_each_L1_WU_per_day_diff1', 'max_sell_price_main_each_name0_WU_per_day_diff1', 'max_sell_price_main_each_L1_WU_per_day_diff1', 'median_sell_price_main_each_name_WU_per_day_diff1', 'median_sell_price_main_each_name0_WU_per_day_diff1', 'median_sell_price_main_each_L1_WU_per_day_diff1', 'max_total_type_discount_each_name0_WU_per_day_diff1', 'max_total_type_discount_each_L1_WU_per_day_diff1', 'median_total_type_discount_each_name_WU_per_day_diff1', 'median_total_type_discount_each_name0_WU_per_day_diff1', 'median_total_type_discount_each_L1_WU_per_day_diff1', 'country_Hungary', 'country_valuecounts']\n",
      "Memory usage of dataframe is 38.32 MB\n",
      "Memory usage after optimization is: 20.43 MB\n",
      "Decreased by 46.7%\n",
      "------------------------------\n",
      "\u001b[34mprediction on test data\u001b[0m\n",
      "train_date_range:[28.0,1098.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because MAE is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 419.5813468\ttotal: 6.9s\tremaining: 3h 55m 17s\n",
      "100:\tlearn: 123.3749234\ttotal: 56.5s\tremaining: 18m 8s\n",
      "200:\tlearn: 113.1361033\ttotal: 1m 47s\tremaining: 16m 25s\n",
      "300:\tlearn: 107.0383564\ttotal: 2m 38s\tremaining: 15m 19s\n",
      "400:\tlearn: 102.6713543\ttotal: 3m 29s\tremaining: 14m 20s\n",
      "500:\tlearn: 99.2636880\ttotal: 4m 21s\tremaining: 13m 27s\n",
      "600:\tlearn: 96.2988313\ttotal: 5m 12s\tremaining: 12m 32s\n",
      "700:\tlearn: 93.8160139\ttotal: 6m 4s\tremaining: 11m 39s\n",
      "800:\tlearn: 91.5203867\ttotal: 6m 55s\tremaining: 10m 47s\n",
      "900:\tlearn: 89.5304655\ttotal: 7m 47s\tremaining: 9m 54s\n",
      "1000:\tlearn: 87.6206686\ttotal: 8m 39s\tremaining: 9m 2s\n",
      "1100:\tlearn: 85.9102004\ttotal: 9m 31s\tremaining: 8m 11s\n",
      "1200:\tlearn: 84.3376741\ttotal: 10m 22s\tremaining: 7m 19s\n",
      "1300:\tlearn: 82.8951073\ttotal: 11m 15s\tremaining: 6m 27s\n",
      "1400:\tlearn: 81.4802666\ttotal: 12m 7s\tremaining: 5m 35s\n",
      "1500:\tlearn: 80.1391247\ttotal: 12m 59s\tremaining: 4m 44s\n",
      "1600:\tlearn: 78.7513873\ttotal: 13m 51s\tremaining: 3m 52s\n",
      "1700:\tlearn: 77.5980464\ttotal: 14m 43s\tremaining: 3m\n",
      "1800:\tlearn: 76.3595331\ttotal: 15m 36s\tremaining: 2m 8s\n",
      "1900:\tlearn: 75.2705013\ttotal: 16m 28s\tremaining: 1m 16s\n",
      "2000:\tlearn: 74.1310834\ttotal: 17m 21s\tremaining: 24.5s\n",
      "2047:\tlearn: 73.6347379\ttotal: 17m 46s\tremaining: 0us\n",
      "\u001b[32msubmission......\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def weighted_MAE(y_true,y_pred,weight):\n",
    "    return np.sum(weight*np.abs(y_true-y_pred))/np.sum(weight)\n",
    "\n",
    "xgb_params={'objective': 'reg:squarederror', 'colsample_bytree': 0.6, \n",
    "            'enable_categorical': True, 'learning_rate': 0.2,  'max_depth': 9, \n",
    "            'n_estimators': 1280,  'random_state': 2024, 'reg_alpha': 0.08,\n",
    "            'reg_lambda': 0.8, 'subsample': 0.95,'tree_method':'gpu_hist'\n",
    "           }\n",
    "cat_params={'random_state':2024,\n",
    "           'eval_metric'         : 'MAE',\n",
    "           'bagging_temperature' : 0.50,\n",
    "           'iterations'          : 2048,\n",
    "           'learning_rate'       : 0.1,\n",
    "           'max_depth'           : 12,\n",
    "           'l2_leaf_reg'         : 1.25,\n",
    "           'min_data_in_leaf'    : 24,\n",
    "           'random_strength'     : 0.25, \n",
    "           'verbose'             : 0,\n",
    "           'task_type':\"GPU\"\n",
    "          }\n",
    "models=[(XGBRegressor(**xgb_params),'xgb'),\n",
    "        (CatBoostRegressor(**cat_params),'cat')\n",
    "       ]\n",
    "\n",
    "yunbase=Yunbase(num_folds=1,\n",
    "                  models=models,\n",
    "                  FE=None,\n",
    "                  seed=2024,\n",
    "                  objective='regression',\n",
    "                  custom_metric=weighted_MAE,\n",
    "                  drop_cols=[],\n",
    "                  target_col='sales',\n",
    "                  save_oof_preds=True,\n",
    "                  save_test_preds=False,\n",
    "                  device='gpu',\n",
    "                  one_hot_max=10,\n",
    "                  early_stop=100,\n",
    "                  cross_cols=['total_orders','sell_price_main','type_0_discount'],\n",
    "                  use_high_corr_feat=False,\n",
    "                  use_reduce_memory=True,\n",
    "                  log=100,\n",
    "                  plot_feature_importance=True,\n",
    ")\n",
    "test_preds=yunbase.purged_cross_validation(train_path_or_file=train,\n",
    "                                           test_path_or_file=test,\n",
    "                                           date_col='date',train_gap_each_fold=28,\n",
    "                                           train_test_gap=0,\n",
    "                                           use_seasonal_features=False,\n",
    "                                           use_weighted_metric=True,\n",
    "                                           category_cols=['unique_id','L2_category_name_en',\n",
    "                                               'L3_category_name_en','L4_category_name_en',\n",
    "                                                'name_0','name_1','product_unique_id'],\n",
    "                                           only_inference=True,\n",
    "                                          )\n",
    "test_preds=np.clip(test_preds,0.0, 26316)\n",
    "test_preds=test_preds*test['dayofweek'].values\n",
    "yunbase.target_col='sales_hat'\n",
    "yunbase.submit(\"/kaggle/input/rohlik-sales-forecasting-challenge-v2/solution.csv\",test_preds)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 10173359,
     "sourceId": 88742,
     "sourceType": "competition"
    },
    {
     "sourceId": 216317351,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1920.484329,
   "end_time": "2025-02-07T12:53:52.861823",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-07T12:21:52.377494",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
